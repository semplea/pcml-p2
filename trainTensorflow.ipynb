{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument --dev_sample_percentage: conflicting option string: --dev_sample_percentage",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-276d478e1401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Data loading params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dev_sample_percentage\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Percentage of the training data to use for validation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"positive_data_file\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_train_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Data source for the positive data.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative_data_file\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_train_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Data source for the positive data.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/baptisteraemy/anaconda3/lib/python3.5/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36mDEFINE_float\u001b[0;34m(flag_name, default_value, docstring)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mdocstring\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mhelpful\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0mexplaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0muse\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m   \u001b[0m_define_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m _allowed_symbols = [\n",
      "\u001b[0;32m/Users/baptisteraemy/anaconda3/lib/python3.5/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36m_define_helper\u001b[0;34m(flag_name, default_value, docstring, flagtype)\u001b[0m\n\u001b[1;32m     63\u001b[0m                               \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                               \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                               type=flagtype)\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/baptisteraemy/anaconda3/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36madd_argument\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1342\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"length of metavar tuple does not match nargs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_argument_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/baptisteraemy/anaconda3/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1705\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption_strings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1707\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optionals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1708\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_positionals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/baptisteraemy/anaconda3/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1548\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ArgumentGroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1549\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/baptisteraemy/anaconda3/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m         \u001b[0;31m# resolve any conflicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_conflict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;31m# add to actions list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/baptisteraemy/anaconda3/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36m_check_conflict\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m             \u001b[0mconflict_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1497\u001b[0;31m             \u001b[0mconflict_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_conflict_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/baptisteraemy/anaconda3/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36m_handle_conflict_error\u001b[0;34m(self, action, conflicting_actions)\u001b[0m\n\u001b[1;32m   1504\u001b[0m                                      \u001b[0;32mfor\u001b[0m \u001b[0moption_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m                                      in conflicting_actions])\n\u001b[0;32m-> 1506\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mArgumentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mconflict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_conflict_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument --dev_sample_percentage: conflicting option string: --dev_sample_percentage"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from functions_helpers import *\n",
    "from TextCNN import *\n",
    "\n",
    "data_folder = 'twitter-datasets/'\n",
    "embeddings_dim = 20\n",
    "pos_train_file = data_folder + 'pos_train.txt'\n",
    "neg_train_file = data_folder + 'neg_train.txt'\n",
    "vocab_pickle = data_folder + 'vocab.pkl'\n",
    "cooc_pickle = data_folder + 'cooc.pkl'\n",
    "embeddings_file = data_folder + 'embeddings.npy'\n",
    "filter_sizes = \"3,4,5\" # must be a string, not array of int\n",
    "num_filters = 128\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", 0.1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", pos_train_file, \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", neg_train_file, \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"vocab_file\", vocab_pickle, \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"embeddings_file\", embeddings_file, \"Data source for the positive data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "X_train shape (200000,) Y_train shape (200000, 2)\n",
      "(21162, 20)\n",
      "21161\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_train, y_train = load_data_label(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "print('X_train shape', x_train.shape,'Y_train shape',  y_train.shape)\n",
    "#np.savetxt('x_train_padded.txt', x_train)\n",
    "#np.savetxt('y_train_padded.txt', y_train)\n",
    "\n",
    "# load_vocab\n",
    "vocab = load_pickle(FLAGS.vocab_file)\n",
    "embeddings = np.load(FLAGS.embeddings_file)\n",
    "\n",
    "#add one fake word embedding for the random\n",
    "embeddings = np.vstack([embeddings, np.ones([1,embeddings.shape[1]])*-999])\n",
    "\n",
    "print(embeddings.shape)\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=20\n",
      "EMBEDDINGS_FILE=twitter-datasets/embeddings.npy\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=twitter-datasets/neg_train.txt\n",
      "NUM_EPOCHS=50\n",
      "NUM_FILTERS=128\n",
      "POSITIVE_DATA_FILE=twitter-datasets/pos_train.txt\n",
      "VOCAB_FILE=twitter-datasets/vocab.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", embeddings_dim, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", filter_sizes, \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", num_filters, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularizaion lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 50, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data:  1000\n",
      "0\n",
      "200000\n",
      "Train/Dev split: 900/100\n"
     ]
    }
   ],
   "source": [
    "# Data Preparatopn\n",
    "# ==================================================\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "x_shuffled = x_train[shuffle_indices]\n",
    "y_shuffled = y_train[shuffle_indices]\n",
    "\n",
    "\n",
    "x_shuffled = map_data(x_shuffled[:1000], vocab)\n",
    "y_shuffled = y_shuffled[:1000]\n",
    "\n",
    "\n",
    "print(len(y_train))\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index =-1 * int(FLAGS.dev_sample_percentage * float(len(y_shuffled)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /Users/baptisteraemy/GoogleDrive/EPFL/Master/Semestre3/PCML/Project/pcml-p2/runs/1481885706\n",
      "\n",
      "2016-12-16T11:55:09.665584: step 10, loss 1.04427, acc 0.546875\n",
      "2016-12-16T11:55:10.430084: step 20, loss 1.06807, acc 0.5\n",
      "2016-12-16T11:55:11.207232: step 30, loss 1.49109, acc 0.75\n",
      "2016-12-16T11:55:12.037161: step 40, loss 1.03272, acc 0.546875\n",
      "2016-12-16T11:55:12.864013: step 50, loss 0.80248, acc 0.59375\n",
      "2016-12-16T11:55:13.674807: step 60, loss 0.747517, acc 0.75\n",
      "2016-12-16T11:55:14.543917: step 70, loss 0.778381, acc 0.609375\n",
      "2016-12-16T11:55:15.364535: step 80, loss 0.775281, acc 0.65625\n",
      "2016-12-16T11:55:16.198291: step 90, loss 0.105675, acc 1\n",
      "2016-12-16T11:55:17.049238: step 100, loss 0.751123, acc 0.578125\n",
      "\n",
      "Evaluation:\n",
      "2016-12-16T11:55:17.099563: step 100, loss 0.547492, acc 0.71\n",
      "\n",
      "Saved model checkpoint to /Users/baptisteraemy/GoogleDrive/EPFL/Master/Semestre3/PCML/Project/pcml-p2/runs/1481885706/checkpoints/model-100\n",
      "\n",
      "2016-12-16T11:55:18.696148: step 110, loss 0.728492, acc 0.703125\n",
      "2016-12-16T11:55:19.625780: step 120, loss 0.427337, acc 0.75\n",
      "2016-12-16T11:55:20.632411: step 130, loss 0.52137, acc 0.71875\n",
      "2016-12-16T11:55:21.856814: step 140, loss 0.61799, acc 0.703125\n",
      "2016-12-16T11:55:22.917727: step 150, loss 0.37873, acc 0.75\n",
      "2016-12-16T11:55:23.888952: step 160, loss 0.525593, acc 0.75\n",
      "2016-12-16T11:55:24.757726: step 170, loss 0.409536, acc 0.78125\n",
      "2016-12-16T11:55:25.711001: step 180, loss 0.35252, acc 0.75\n",
      "2016-12-16T11:55:26.834319: step 190, loss 0.660911, acc 0.734375\n",
      "2016-12-16T11:55:27.902966: step 200, loss 0.645073, acc 0.671875\n",
      "\n",
      "Evaluation:\n",
      "2016-12-16T11:55:27.953169: step 200, loss 0.591457, acc 0.66\n",
      "\n",
      "Saved model checkpoint to /Users/baptisteraemy/GoogleDrive/EPFL/Master/Semestre3/PCML/Project/pcml-p2/runs/1481885706/checkpoints/model-200\n",
      "\n",
      "2016-12-16T11:55:29.657712: step 210, loss 0.462885, acc 0.75\n",
      "2016-12-16T11:55:30.701803: step 220, loss 0.462512, acc 0.71875\n",
      "2016-12-16T11:55:31.620141: step 230, loss 0.493221, acc 0.78125\n",
      "2016-12-16T11:55:32.520877: step 240, loss 0.114373, acc 1\n",
      "2016-12-16T11:55:33.415598: step 250, loss 0.453242, acc 0.78125\n",
      "2016-12-16T11:55:34.262010: step 260, loss 0.580791, acc 0.65625\n",
      "2016-12-16T11:55:35.129249: step 270, loss 0.429271, acc 0.5\n",
      "2016-12-16T11:55:36.016519: step 280, loss 0.340665, acc 0.828125\n",
      "2016-12-16T11:55:36.941853: step 290, loss 0.419826, acc 0.859375\n",
      "2016-12-16T11:55:37.761633: step 300, loss 0.642078, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2016-12-16T11:55:37.805197: step 300, loss 0.521702, acc 0.73\n",
      "\n",
      "Saved model checkpoint to /Users/baptisteraemy/GoogleDrive/EPFL/Master/Semestre3/PCML/Project/pcml-p2/runs/1481885706/checkpoints/model-300\n",
      "\n",
      "2016-12-16T11:55:39.441304: step 310, loss 0.280029, acc 0.875\n",
      "2016-12-16T11:55:40.288512: step 320, loss 0.335108, acc 0.84375\n",
      "2016-12-16T11:55:41.129501: step 330, loss 0.387375, acc 0.75\n",
      "2016-12-16T11:55:42.016772: step 340, loss 0.356544, acc 0.75\n",
      "2016-12-16T11:55:42.853805: step 350, loss 0.340988, acc 0.875\n",
      "2016-12-16T11:55:43.680683: step 360, loss 0.128555, acc 1\n",
      "2016-12-16T11:55:44.554390: step 370, loss 0.310544, acc 0.890625\n",
      "2016-12-16T11:55:45.384879: step 380, loss 0.341295, acc 0.828125\n",
      "2016-12-16T11:55:46.215851: step 390, loss 0.363486, acc 0.75\n",
      "2016-12-16T11:55:47.068246: step 400, loss 0.222729, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2016-12-16T11:55:47.113158: step 400, loss 0.519997, acc 0.77\n",
      "\n",
      "Saved model checkpoint to /Users/baptisteraemy/GoogleDrive/EPFL/Master/Semestre3/PCML/Project/pcml-p2/runs/1481885706/checkpoints/model-400\n",
      "\n",
      "2016-12-16T11:55:48.684584: step 410, loss 0.180011, acc 0.921875\n",
      "2016-12-16T11:55:49.517708: step 420, loss 0.0607985, acc 1\n",
      "2016-12-16T11:55:50.388063: step 430, loss 0.267237, acc 0.90625\n",
      "2016-12-16T11:55:51.211490: step 440, loss 0.344903, acc 0.84375\n",
      "2016-12-16T11:55:52.026344: step 450, loss 0.339912, acc 0.75\n",
      "2016-12-16T11:55:52.894519: step 460, loss 0.227348, acc 0.90625\n",
      "2016-12-16T11:55:53.704792: step 470, loss 0.218129, acc 0.90625\n",
      "2016-12-16T11:55:54.524577: step 480, loss 0.0925522, acc 1\n",
      "2016-12-16T11:55:55.384354: step 490, loss 0.243948, acc 0.890625\n",
      "2016-12-16T11:55:56.209397: step 500, loss 0.277524, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2016-12-16T11:55:56.259906: step 500, loss 0.514663, acc 0.74\n",
      "\n",
      "Saved model checkpoint to /Users/baptisteraemy/GoogleDrive/EPFL/Master/Semestre3/PCML/Project/pcml-p2/runs/1481885706/checkpoints/model-500\n",
      "\n",
      "2016-12-16T11:55:58.001456: step 510, loss 1.02323, acc 0.5\n",
      "2016-12-16T11:55:58.945237: step 520, loss 0.22534, acc 0.890625\n",
      "2016-12-16T11:55:59.859779: step 530, loss 0.230732, acc 0.90625\n",
      "2016-12-16T11:56:00.768326: step 540, loss 0.529708, acc 0.5\n",
      "2016-12-16T11:56:01.736310: step 550, loss 0.193026, acc 0.921875\n",
      "2016-12-16T11:56:02.629422: step 560, loss 0.0991772, acc 0.953125\n",
      "2016-12-16T11:56:03.527053: step 570, loss 0.0476257, acc 1\n",
      "2016-12-16T11:56:04.498778: step 580, loss 0.180391, acc 0.953125\n",
      "2016-12-16T11:56:05.411360: step 590, loss 0.185242, acc 0.890625\n",
      "2016-12-16T11:56:06.266278: step 600, loss 0.246017, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2016-12-16T11:56:06.312606: step 600, loss 0.554451, acc 0.74\n",
      "\n",
      "Saved model checkpoint to /Users/baptisteraemy/GoogleDrive/EPFL/Master/Semestre3/PCML/Project/pcml-p2/runs/1481885706/checkpoints/model-600\n",
      "\n",
      "2016-12-16T11:56:07.950159: step 610, loss 0.14605, acc 0.953125\n",
      "2016-12-16T11:56:08.839687: step 620, loss 0.102888, acc 0.96875\n",
      "2016-12-16T11:56:09.753476: step 630, loss 0.543493, acc 0.5\n",
      "2016-12-16T11:56:10.736680: step 640, loss 0.081465, acc 1\n",
      "2016-12-16T11:56:11.695435: step 650, loss 0.103896, acc 0.96875\n",
      "2016-12-16T11:56:12.568108: step 660, loss 0.403476, acc 0.75\n",
      "2016-12-16T11:56:13.467496: step 670, loss 0.106866, acc 0.96875\n",
      "2016-12-16T11:56:14.493532: step 680, loss 0.104251, acc 0.953125\n",
      "2016-12-16T11:56:15.479910: step 690, loss 0.0181706, acc 1\n",
      "2016-12-16T11:56:16.364835: step 700, loss 0.103102, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2016-12-16T11:56:16.411067: step 700, loss 0.559684, acc 0.73\n",
      "\n",
      "Saved model checkpoint to /Users/baptisteraemy/GoogleDrive/EPFL/Master/Semestre3/PCML/Project/pcml-p2/runs/1481885706/checkpoints/model-700\n",
      "\n",
      "2016-12-16T11:56:18.011624: step 710, loss 0.163718, acc 0.921875\n",
      "2016-12-16T11:56:18.893955: step 720, loss 0.0230729, acc 1\n",
      "2016-12-16T11:56:19.766442: step 730, loss 0.114827, acc 0.96875\n",
      "2016-12-16T11:56:20.578323: step 740, loss 0.133336, acc 0.953125\n",
      "2016-12-16T11:56:21.396483: step 750, loss 0.0962597, acc 1\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length= 64, # TODO change hardcoding\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab) + 1, # the + 1 is for the <pad> token\n",
    "            embedding_dim= embeddings.shape[1],\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "        # Write vocabulary\n",
    "        #vocab_processor.save(os.path.join(out_dir, \"vocab\")) vocab already exists\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.now().isoformat()\n",
    "            if step %10 == 0:\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.now().isoformat()\n",
    "            if step %10 == 0:\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
